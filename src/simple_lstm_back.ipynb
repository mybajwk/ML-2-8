{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78860a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample vocab: ['', '[UNK]', np.str_('yang'), np.str_('di'), np.str_('dan'), np.str_('tidak'), np.str_('saya'), np.str_('dengan'), np.str_('enak'), np.str_('ini')]\n",
      "Max index: 2836\n",
      "\n",
      "=== Running Keras-Best: {'n_layers': 3, 'units': [64, 128, 256], 'bidirectional': True}\n",
      "Epoch 1/5\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 172ms/step - loss: 1.0486 - val_loss: 1.0145\n",
      "Epoch 2/5\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 160ms/step - loss: 0.8537 - val_loss: 0.8182\n",
      "Epoch 3/5\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 155ms/step - loss: 0.5560 - val_loss: 0.9192\n",
      "Epoch 4/5\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 152ms/step - loss: 0.3558 - val_loss: 1.1204\n",
      "Epoch 5/5\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - loss: 0.2509 - val_loss: 1.1578\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step\n",
      "Keras-Best macro-F1: 0.6050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:18: RuntimeWarning: divide by zero encountered in matmul\n",
      "  z = xt @ self.W + H @ self.U + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:18: RuntimeWarning: overflow encountered in matmul\n",
      "  z = xt @ self.W + H @ self.U + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:18: RuntimeWarning: invalid value encountered in matmul\n",
      "  z = xt @ self.W + H @ self.U + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:10: RuntimeWarning: divide by zero encountered in matmul\n",
      "  z = x @ self.W + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:10: RuntimeWarning: overflow encountered in matmul\n",
      "  z = x @ self.W + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:10: RuntimeWarning: invalid value encountered in matmul\n",
      "  z = x @ self.W + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:18: RuntimeWarning: divide by zero encountered in matmul\n",
      "  z = xt @ self.W + H @ self.U + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:18: RuntimeWarning: overflow encountered in matmul\n",
      "  z = xt @ self.W + H @ self.U + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:18: RuntimeWarning: invalid value encountered in matmul\n",
      "  z = xt @ self.W + H @ self.U + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:10: RuntimeWarning: divide by zero encountered in matmul\n",
      "  z = x @ self.W + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:10: RuntimeWarning: overflow encountered in matmul\n",
      "  z = x @ self.W + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:10: RuntimeWarning: invalid value encountered in matmul\n",
      "  z = x @ self.W + self.b\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:18: RuntimeWarning: divide by zero encountered in matmul\n",
      "  self.dW = x.T @ dL_dz\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:18: RuntimeWarning: overflow encountered in matmul\n",
      "  self.dW = x.T @ dL_dz\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:18: RuntimeWarning: invalid value encountered in matmul\n",
      "  self.dW = x.T @ dL_dz\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:20: RuntimeWarning: divide by zero encountered in matmul\n",
      "  dL_dx = dL_dz @ self.W.T\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:20: RuntimeWarning: overflow encountered in matmul\n",
      "  dL_dx = dL_dz @ self.W.T\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/dense_layer.py:20: RuntimeWarning: invalid value encountered in matmul\n",
      "  dL_dx = dL_dz @ self.W.T\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:58: RuntimeWarning: divide by zero encountered in matmul\n",
      "  dW += xt.T @ dz\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:58: RuntimeWarning: overflow encountered in matmul\n",
      "  dW += xt.T @ dz\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:58: RuntimeWarning: invalid value encountered in matmul\n",
      "  dW += xt.T @ dz\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:59: RuntimeWarning: divide by zero encountered in matmul\n",
      "  dU += self.cache[t - 1][1].T @ dz if t > 0 else 0\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:59: RuntimeWarning: overflow encountered in matmul\n",
      "  dU += self.cache[t - 1][1].T @ dz if t > 0 else 0\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:59: RuntimeWarning: invalid value encountered in matmul\n",
      "  dU += self.cache[t - 1][1].T @ dz if t > 0 else 0\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:62: RuntimeWarning: divide by zero encountered in matmul\n",
      "  dX_step = dz @ self.W.T\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:62: RuntimeWarning: overflow encountered in matmul\n",
      "  dX_step = dz @ self.W.T\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:62: RuntimeWarning: invalid value encountered in matmul\n",
      "  dX_step = dz @ self.W.T\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:63: RuntimeWarning: divide by zero encountered in matmul\n",
      "  dH_next = dz @ self.U.T\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  dH_next = dz @ self.U.T\n",
      "/Users/enrique/Projects/Coding/ML-2-8/src/models/lstm/lstm_layer.py:63: RuntimeWarning: invalid value encountered in matmul\n",
      "  dH_next = dz @ self.U.T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "macro-F1 (scratch - forward only): 0.6050\n",
      "[Epoch 1] Scratch Loss: 10.2723\n",
      "[Epoch 2] Scratch Loss: 10.2415\n",
      "[Epoch 3] Scratch Loss: 10.2081\n",
      "[Epoch 4] Scratch Loss: 10.1949\n",
      "[Epoch 5] Scratch Loss: 10.1580\n",
      "[Epoch 6] Scratch Loss: 10.1229\n",
      "[Epoch 7] Scratch Loss: 10.1169\n",
      "[Epoch 8] Scratch Loss: 10.0812\n",
      "[Epoch 9] Scratch Loss: 10.0625\n",
      "[Epoch 10] Scratch Loss: 10.0600\n",
      "\n",
      "macro-F1 (scratch - trained): 0.6104\n"
     ]
    }
   ],
   "source": [
    "# === HEADER ===\n",
    "import os, random, numpy as np, pandas as pd, tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from models.lstm.keras_model import build_lstm_model\n",
    "from models.lstm.scratch_model import ScratchLSTMClassifier\n",
    "\n",
    "# === SEEDING ===\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# === DATA LOAD ===\n",
    "train = pd.read_csv('./../data/nusaX-sentiment/train.csv')\n",
    "valid = pd.read_csv('./../data/nusaX-sentiment/valid.csv')\n",
    "test = pd.read_csv('./../data/nusaX-sentiment/test.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train['label'])\n",
    "y_val = le.transform(valid['label'])\n",
    "y_test = le.transform(test['label'])\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "texts_train = train['text'].astype(str).tolist()\n",
    "texts_val = valid['text'].astype(str).tolist()\n",
    "texts_test = test['text'].astype(str).tolist()\n",
    "\n",
    "max_tokens = 20000\n",
    "max_len = 100\n",
    "embed_dim = 128\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=max_len)\n",
    "vectorizer.adapt(texts_train)\n",
    "\n",
    "def make_dataset(texts, labels, batch=32, shuffle=True):\n",
    "    x = vectorizer(tf.constant(texts))\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, labels))\n",
    "    if shuffle: ds = ds.shuffle(1024)\n",
    "    return ds.batch(batch).prefetch(1)\n",
    "\n",
    "print(\"Sample vocab:\", vectorizer.get_vocabulary()[:10])\n",
    "print(\"Max index:\", len(vectorizer.get_vocabulary()))\n",
    "\n",
    "ds_train = make_dataset(texts_train, y_train)\n",
    "ds_val = make_dataset(texts_val, y_val, shuffle=False)\n",
    "ds_test = make_dataset(texts_test, y_test, shuffle=False)\n",
    "\n",
    "# === KERAS TRAINING ===\n",
    "def train_and_eval(params, name):\n",
    "    print(f\"\\n=== Running {name}: {params}\")\n",
    "    model = build_lstm_model(**params, max_len=max_len, max_tokens=max_tokens,\n",
    "                             embed_dim=embed_dim, num_classes=num_classes)\n",
    "    model.fit(ds_train, validation_data=ds_val, epochs=5)\n",
    "    y_pred = np.argmax(model.predict(ds_test), axis=-1)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f\"{name} macro-F1: {f1:.4f}\")\n",
    "    return model\n",
    "\n",
    "params = {'n_layers': 3, 'units': [64, 128, 256], 'bidirectional': True}\n",
    "best_model = train_and_eval(params, \"Keras-Best\")\n",
    "best_model.save_weights(\"best_lstm_back.weights.h5\")\n",
    "\n",
    "# === UNPACK & CONVERT TO SCRATCH ===\n",
    "def unpack_lstm(layer):\n",
    "    if isinstance(layer, tf.keras.layers.Bidirectional):\n",
    "        return_seq = layer.forward_layer.return_sequences\n",
    "        Wf, Uf, bf = layer.forward_layer.get_weights()\n",
    "        Wb, Ub, bb = layer.backward_layer.get_weights()\n",
    "        return ('bidir', return_seq, (Wf, Uf, bf), (Wb, Ub, bb))\n",
    "    else:\n",
    "        return_seq = layer.return_sequences\n",
    "        W, U, b = layer.get_weights()\n",
    "        return ('unidir', return_seq, (W, U, b))\n",
    "\n",
    "emb_w = best_model.layers[1].get_weights()[0]\n",
    "d_w, d_b = best_model.layers[-1].get_weights()\n",
    "lstm_layers = [ly for ly in best_model.layers if isinstance(ly, (tf.keras.layers.LSTM, tf.keras.layers.Bidirectional))]\n",
    "scratch_specs = [unpack_lstm(ly) for ly in lstm_layers]\n",
    "\n",
    "scratch_model = ScratchLSTMClassifier(emb_w, scratch_specs, d_w, d_b)\n",
    "\n",
    "# === FORWARD SCRATCH EVALUATION ===\n",
    "x_test_int = vectorizer(tf.constant(texts_test)).numpy()\n",
    "pred_scratch = scratch_model.forward(x_test_int)\n",
    "yhat_s = np.argmax(pred_scratch, axis=1)\n",
    "f1_forward = f1_score(y_test, yhat_s, average='macro')\n",
    "print(f\"\\nmacro-F1 (scratch - forward only): {f1_forward:.4f}\")\n",
    "\n",
    "# === BACKWARD MANUAL TRAINING SCRATCH ===\n",
    "def softmax_cross_entropy_loss(logits, labels):\n",
    "    y_onehot = np.eye(num_classes)[labels]\n",
    "    exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    probs = exps / exps.sum(axis=1, keepdims=True)\n",
    "    loss = -np.sum(y_onehot * np.log(probs + 1e-8)) / len(labels)\n",
    "    grad = (probs - y_onehot) / len(labels)\n",
    "    return loss, grad\n",
    "\n",
    "x_train_int = vectorizer(tf.constant(texts_train)).numpy()\n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "\n",
    "for ep in range(epochs):\n",
    "    idx = np.random.permutation(len(x_train_int))\n",
    "    x_train_int = x_train_int[idx]\n",
    "    y_train = y_train[idx]\n",
    "    ep_loss = 0\n",
    "    for i in range(0, len(x_train_int), batch_size):\n",
    "        xb = x_train_int[i:i+batch_size]\n",
    "        yb = y_train[i:i+batch_size]\n",
    "        logits = scratch_model.forward(xb)\n",
    "        loss, grad = softmax_cross_entropy_loss(logits, yb)\n",
    "        scratch_model.backward(grad)\n",
    "        scratch_model.dense.W -= lr * scratch_model.dense.dW\n",
    "        scratch_model.dense.b -= lr * scratch_model.dense.db\n",
    "        for spec in scratch_model.lstm_specs:\n",
    "            if spec[0] == 'unidir':\n",
    "                lstm = scratch_model.lstm_layers.pop(0)[1]\n",
    "                lstm.W -= lr * lstm.dW\n",
    "                lstm.U -= lr * lstm.dU\n",
    "                lstm.b -= lr * lstm.db\n",
    "            else:\n",
    "                f_lstm, b_lstm = scratch_model.lstm_layers.pop(0)[1:]\n",
    "                for lstm in [f_lstm, b_lstm]:\n",
    "                    lstm.W -= lr * lstm.dW\n",
    "                    lstm.U -= lr * lstm.dU\n",
    "                    lstm.b -= lr * lstm.db\n",
    "        scratch_model.embedding.W -= lr * scratch_model.embedding.dW\n",
    "        ep_loss += loss\n",
    "    print(f\"[Epoch {ep+1}] Scratch Loss: {ep_loss:.4f}\")\n",
    "\n",
    "# === FINAL EVALUATION ===\n",
    "pred_final = scratch_model.forward(x_test_int)\n",
    "yhat_final = np.argmax(pred_final, axis=1)\n",
    "f1_final = f1_score(y_test, yhat_final, average='macro')\n",
    "print(f\"\\nmacro-F1 (scratch - trained): {f1_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b646f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
